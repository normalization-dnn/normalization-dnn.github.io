---
layout: page
title: CVPR 2021 TUTORIALS
subtitle: Normalization Techniques in Deep Learning Methods, Analyses, and Applications
---

## **Course description**
Normalization methods can improve the training stability, optimization efficiency and generalization ability of deep neural networks (DNNs), and have become basic components in most state-of-the-art DNN architectures. They also have successfully proliferated throughout various areas of deep learning, including but not limited to computer vision, natural language processing, and speech recognition. However, despite the abundance and ever more important roles of normalization techniques, we note that there is an absence of a unifying lens with which to describe, compare and analyze them. In addition, our understanding of theoretical foundations of these methods for their success remain elusive. This tutorial cover normalization methods, analyses and applications, and will address the following questions:

(1) What are the main motivations behind different normalization methods in DNNs, and how can we present a taxonomy for understanding the similarities and differences between a wide variety of approaches?

(2) How can we reduce the gap between the empirical success of normalization techniques and our theoretical understanding of them?

(3) What recent advances have been made in designing/tailoring normalization techniques for different tasks, and what are the main insights behind them?



**A complete outline and details:**



- I gave a keynote at [Baltic DB&IS 2020](https://dbis.ttu.ee/) to convince the audience to make BERT their new IR baseline: [slides](https://docs.google.com/presentation/d/e/2PACX-1vQu5aCFGV_cvTbt8fXMf_CIDPbVjBJdYGSYHbi4D126hO8Zqv6jwrEpQAHGm9uQfIKGxMPpQ1HVkP1P/pub?start=false&loop=false&delayms=3000)!

